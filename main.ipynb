{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngseo0526/GoodCasting/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw89c67xp3Sr",
        "outputId": "9b68bcd3-225f-400f-b8c7-12bc1b4afaef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otojumi_qKHp"
      },
      "source": [
        "### 1. Move path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qusE-X3kEn26",
        "outputId": "1ab902cb-0875-4b09-e253-9de80618e785"
      },
      "source": [
        "#cd /content/drive/MyDrive/Colab Notebooks/NLP_project/GoodCasting/GoodCasting\n",
        "cd [yourpath]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NLP_project/GoodCasting/GoodCasting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ARUvivK7wJP2",
        "outputId": "c1588a94-08fa-477c-8d44-f84611912cf2"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/NLP_project/GoodCasting/GoodCasting'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olk35UDU2ovw"
      },
      "source": [
        "### 2. Download pt file\n",
        "- StyleGAN Weights<br>\n",
        "Download the `.pt` file from [here](https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\n",
        ") and store it inside the folder of this repo with the name `karras2019stylegan-ffhq-1024x1024.for_g_all.pt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHgzqjqVoALu"
      },
      "source": [
        "### 3. Install CLIP package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkNgsvrBkMra",
        "outputId": "3b474f00-a3ad-4930-f382-b8109307653a"
      },
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ut2k6jja\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-ut2k6jja\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369106 sha256=3a2f2cdac50905e0adf365119e8e11835596d829174639887979748e1f7e458e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bytskkqc/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=436af658dfe683f0a35235a2fc93cf0bd8cae0675aa5e039611b018e51d8dbc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built clip ftfy\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ5lF6zooALx"
      },
      "source": [
        "### 4. Install T5 package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hALPDUUrk8Ck",
        "outputId": "c26cc709-3cde-48af-f43f-a107e87cac65"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 31.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV_D7tF3piM7"
      },
      "source": [
        "####5. run main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-n3vKTUWFsn",
        "outputId": "afdcc998-1a70-4405-ede8-65a212001ed7"
      },
      "source": [
        "#@title Input text\n",
        "input_text= \"This person is a middle-aged woman. She has light brown short hair. Her forehead is a bit wide. She has light eyebrows and double eyelids. She has black pupils and has wrinkles around her eyes. Her nose is round and large. Her lips are pink, thin and small. She is wearing glasses. She is wearing only one earring. Her face shape is round.\" #@param {type: \"string\"}\n",
        "\n",
        "!python clip_generate.py --prompt input_text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your max_length is set to 100, but you input_length is only 87. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
            "<summarization> \n",
            " middle-aged woman has light brown short hair, eyebrows and double eyelids . her nose is round and large, her lips are pink, thin and small . she is wearing glasses and is wearing only one earring .\n",
            "USING  cuda\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3847: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n",
            "Step 0\n",
            "Loss 3.8359375\n",
            "Step 5\n",
            "Loss 3.75390625\n",
            "Step 10\n",
            "Loss 3.40625\n",
            "Step 15\n",
            "Loss 3.4296875\n",
            "Step 20\n",
            "Loss 3.142578125\n",
            "Step 25\n",
            "Loss 3.23828125\n",
            "Step 30\n",
            "Loss 3.373046875\n",
            "Step 35\n",
            "Loss 3.296875\n",
            "Step 40\n",
            "Loss 2.681640625\n",
            "Step 45\n",
            "Loss 2.7421875\n",
            "Step 50\n",
            "Loss 2.591796875\n",
            "Step 55\n",
            "Loss 2.640625\n",
            "Step 60\n",
            "Loss 2.56640625\n",
            "Step 65\n",
            "Loss 2.5078125\n",
            "Step 70\n",
            "Loss 2.501953125\n",
            "Step 75\n",
            "Loss 2.619140625\n",
            "Step 80\n",
            "Loss 2.56640625\n",
            "Step 85\n",
            "Loss 2.54296875\n",
            "Step 90\n",
            "Loss 2.591796875\n",
            "Step 95\n",
            "Loss 2.634765625\n",
            "Step 100\n",
            "Loss 2.4296875\n",
            "Step 105\n",
            "Loss 2.470703125\n",
            "Step 110\n",
            "Loss 2.453125\n",
            "Step 115\n",
            "Loss 2.474609375\n",
            "Step 120\n",
            "Loss 2.419921875\n",
            "Step 125\n",
            "Loss 2.41015625\n",
            "Step 130\n",
            "Loss 2.396484375\n",
            "Step 135\n",
            "Loss 2.515625\n",
            "Step 140\n",
            "Loss 2.40625\n",
            "Step 145\n",
            "Loss 2.451171875\n",
            "Step 150\n",
            "Loss 2.458984375\n",
            "Step 155\n",
            "Loss 2.353515625\n",
            "Traceback (most recent call last):\n",
            "  File \"clip_generate.py\", line 178, in <module>\n",
            "    loss = compute_clip_loss(img, prompt)\n",
            "  File \"clip_generate.py\", line 147, in compute_clip_loss\n",
            "    img_logits, _text_logits = clip_model(img, tokenized_text)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/clip/model.py\", line 356, in forward\n",
            "    text_features = self.encode_text(text)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/clip/model.py\", line 344, in encode_text\n",
            "    x = self.transformer(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/clip/model.py\", line 199, in forward\n",
            "    return self.resblocks(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/clip/model.py\", line 186, in forward\n",
            "    x = x + self.attention(self.ln_1(x))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/clip/model.py\", line 183, in attention\n",
            "    return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\", line 1010, in forward\n",
            "    attn_mask=attn_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 5101, in multi_head_attention_forward\n",
            "    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 4846, in _scaled_dot_product_attention\n",
            "    attn += attn_mask\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}